{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.special import expit  # Sigmoid function\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return expit(x)    \n",
    "\n",
    "# Define the linear activation function\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        \"\"\"\n",
    "        Initialize the neural network\n",
    "        :param input_size: number of input neurons\n",
    "        :param hidden_size: number of hidden neurons\n",
    "        :param output_size: number of output neurons\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # # Initialize weights and biases using Xavier initialization\n",
    "        # self.v = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / (input_size + hidden_size))\n",
    "        # self.v0 = np.zeros(hidden_size)\n",
    "        # self.w = np.random.randn(output_size, hidden_size) * np.sqrt(2.0 / (hidden_size + output_size))\n",
    "        # self.w0 = np.zeros(output_size)\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.v = np.random.rand(hidden_size, input_size)\n",
    "        self.v0 = np.random.rand(hidden_size)\n",
    "        self.w = np.random.rand(output_size, hidden_size)\n",
    "        self.w0 = np.random.rand(output_size)\n",
    "\n",
    "    def predict(self, phi: np.ndarray) -> np.ndarray:   \n",
    "        \"\"\"\n",
    "        Make a prediction with the neural network\n",
    "        :param phi: input data\n",
    "        :return: prediction\n",
    "        \"\"\"\n",
    "        hidden_input = np.dot(self.v, phi) + self.v0\n",
    "        hidden_output = relu(hidden_input)\n",
    "        output = np.dot(self.w, hidden_output) + self.w0\n",
    "        return linear(output)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return np.concatenate([self.v.flatten(), self.v0, self.w.flatten(), self.w0])\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        v_end = self.hidden_size * self.input_size\n",
    "        v0_end = v_end + self.hidden_size\n",
    "        w_end = v0_end + (self.output_size * self.hidden_size)\n",
    "\n",
    "        self.v = weights[:v_end].reshape(self.hidden_size, self.input_size)\n",
    "        self.v0 = weights[v_end:v0_end]\n",
    "        self.w = weights[v0_end:w_end].reshape(self.output_size, self.hidden_size)\n",
    "        self.w0 = weights[w_end:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Define the fitness function\n",
    "def fitness(network, data, targets):\n",
    "    predictions = np.array([network.predict(phi) for phi in data])\n",
    "    rmse = root_mean_squared_error(targets, predictions)\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSO:\n",
    "    def __init__(self, network, data, targets, num_particles, max_iter, w=0.5, c1=2.0, c2=2.0):\n",
    "        self.network = network\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.num_particles = num_particles\n",
    "        self.max_iter = max_iter\n",
    "        self.w = w\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "\n",
    "        self.dim = len(network.get_weights())\n",
    "        self.swarm = np.random.rand(num_particles, self.dim)\n",
    "        self.velocity = np.zeros((num_particles, self.dim))\n",
    "        self.pbest = self.swarm.copy()\n",
    "        self.lbest = self.swarm.copy()\n",
    "        self.pbest_fitness = np.array([self.evaluate_fitness(weights) for weights in self.swarm])\n",
    "        self.lbest_fitness = self.pbest_fitness.copy()\n",
    "        self.history = []\n",
    "\n",
    "    def evaluate_fitness(self, weights):\n",
    "        self.network.set_weights(weights)\n",
    "        return fitness(self.network, self.data, self.targets)\n",
    "\n",
    "    def calculate_diversity(self):\n",
    "        return np.mean(np.linalg.norm(self.swarm - np.mean(self.swarm, axis=0), axis=1))\n",
    "\n",
    "    def adaptive_neighborhood(self, diversity):\n",
    "        neighborhood_size = min(max(2, int(self.num_particles * (1 - diversity))), self.num_particles)\n",
    "        for i in range(self.num_particles):\n",
    "            distances = np.linalg.norm(self.swarm - self.swarm[i], axis=1)\n",
    "            sorted_indices = np.argsort(distances)\n",
    "            neighborhood_indices = sorted_indices[:neighborhood_size]\n",
    "            best_neighbor_index = np.argmin(self.pbest_fitness[neighborhood_indices])\n",
    "            self.lbest[i] = self.pbest[neighborhood_indices[best_neighbor_index]]\n",
    "            self.lbest_fitness[i] = self.pbest_fitness[neighborhood_indices[best_neighbor_index]]\n",
    "\n",
    "    def optimize(self):\n",
    "        for t in range(self.max_iter):\n",
    "            for i in range(self.num_particles):\n",
    "                self.network.set_weights(self.swarm[i])\n",
    "                current_fitness = fitness(self.network, self.data, self.targets)\n",
    "                \n",
    "                # Update personal best\n",
    "                if current_fitness < self.pbest_fitness[i]:\n",
    "                    self.pbest[i] = self.swarm[i].copy()\n",
    "                    self.pbest_fitness[i] = current_fitness\n",
    "\n",
    "            # Calculate swarm diversity\n",
    "            diversity = self.calculate_diversity()\n",
    "\n",
    "            # Adaptive neighborhood adjustment\n",
    "            self.adaptive_neighborhood(diversity)\n",
    "\n",
    "            for i in range(self.num_particles):\n",
    "                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n",
    "                self.velocity[i] = (self.w * self.velocity[i] +\n",
    "                                    self.c1 * r1 * (self.pbest[i] - self.swarm[i]) +\n",
    "                                    self.c2 * r2 * (self.lbest[i] - self.swarm[i]))\n",
    "                self.swarm[i] += self.velocity[i]\n",
    "\n",
    "            # Update the best weights found\n",
    "            best_particle = np.argmin(self.pbest_fitness)\n",
    "            best_weights = self.pbest[best_particle]\n",
    "            self.network.set_weights(best_weights)\n",
    "            best_rmse = fitness(self.network, self.data, self.targets)\n",
    "            print(f'Iteration {t+1}/{self.max_iter} - RMSE: {best_rmse}', end='\\r')\n",
    "            self.history.append(best_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "csv_filename = 'GlobalTemperatures.csv'\n",
    "data = pd.read_csv(csv_filename)\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading CSV file...\")\n",
    "data = pd.read_csv(csv_filename)\n",
    "print(\"Dataset loaded\")\n",
    "\n",
    "# Preprocess the data\n",
    "print(\"Preprocessing data...\")\n",
    "data = data.dropna(subset=['LandAverageTemperature', 'LandAverageTemperatureUncertainty'])\n",
    "\n",
    "# Convert date to datetime\n",
    "data['dt'] = pd.to_datetime(data['dt'])\n",
    "\n",
    "def create_lag_features(data, lags=1, targets=['LandAverageTemperature']):\n",
    "    df = pd.DataFrame(data)\n",
    "    columns = [df[targets].shift(i) for i in range(lags, 0, -1)]\n",
    "    columns = [df] + columns\n",
    "    df = pd.concat(columns, axis=1)\n",
    "    df.dropna(inplace=True)\n",
    "    # df.columns = ['dt', target] + [f'{target}_lag_{i}' for i in range(1, lags + 1)]\n",
    "    df.columns = ['dt'] + targets + [f'{target}_lag_{i}' for target in targets for i in range(1, lags + 1)]\n",
    "    return df\n",
    "\n",
    "# Creating time series\n",
    "# Using LandAverageTemperatureUncertainty as exogenous variable\n",
    "ts = data[['dt', 'LandAverageTemperature']].dropna().copy()\n",
    "\n",
    "# Create lag features\n",
    "lags = 5  # Number of lag observations\n",
    "data_lagged = create_lag_features(ts, lags)\n",
    "\n",
    "# Select features and target variable\n",
    "X = data_lagged.drop(['dt', 'LandAverageTemperature'], axis=1)\n",
    "y = data_lagged['LandAverageTemperature']\n",
    "\n",
    "# Split the data using TimeSeriesSplit\n",
    "time_split = TimeSeriesSplit(n_splits=4)\n",
    "train_index, test_index = list(time_split.split(X))[-1]\n",
    "X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "print(f\"Percent of data in training set: {len(X_train) / len(X) * 100:.2f}%\")\n",
    "print(f\"Percent of data in test set: {len(X_test) / len(X) * 100:.2f}%\")\n",
    "\n",
    "# Standardize the features\n",
    "print(\"Standardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(\"Standardization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network structure\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 45\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network\n",
    "network = NeuralNetwork(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network using PSO\n",
    "pso = PSO(network, X_train, y_train, num_particles=50, max_iter=300, w=0.7, c1=2, c2=1.2)\n",
    "pso.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([network.predict(phi) for phi in X_test])\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_plot_train['dt'] = data_lagged[data_lagged.index < X_plot_train.shape[0]]['dt']\n",
    "X_plot_train['LandAverageTemperature'] = y_train\n",
    "\n",
    "X_plot_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "X_plot_test['dt'] = data_lagged[data_lagged.index >= X_plot_train.shape[0]]['dt'].reset_index(drop=True)\n",
    "X_plot_test['LandAverageTemperature'] = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "X_plot_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_plot_train['dt'] = data_lagged[data_lagged.index < X_plot_train.shape[0]]['dt']\n",
    "X_plot_train['LandAverageTemperature'] = y_train\n",
    "\n",
    "X_plot_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "X_plot_test['dt'] = data_lagged[data_lagged.index >= X_plot_train.shape[0]]['dt'].reset_index(drop=True)\n",
    "X_plot_test['LandAverageTemperature'] = y_test.reset_index(drop=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=X_plot_train['dt'], y=X_plot_train['LandAverageTemperature'], mode='lines', name='Train'))\n",
    "fig.add_trace(go.Scatter(x=X_plot_test['dt'], y=X_plot_test['LandAverageTemperature'], mode='lines', name='Test', opacity=0.7))\n",
    "fig.add_trace(go.Scatter(x=X_plot_test['dt'], y=pd.Series(y_pred.flatten()), mode='lines', name='Pred.'))\n",
    "fig.update_layout(title=\"Predição Climática (PSO | N=3)\", yaxis_title=\"Temperatura Média (°C)\", xaxis_title=\"Data\")\n",
    "fig.show('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history = pd.DataFrame(pso.history, columns=['RMSE'])\n",
    "df_history.to_csv(\"pso_results/PSO_N5_9.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N=5\n",
    "1 0.9100\n",
    "2 1.2137\n",
    "3 0.7949\n",
    "4 0.9215\n",
    "5 0.9202\n",
    "6 0.9383\n",
    "\n",
    "n=3\n",
    "1 0.8696\n",
    "2 0.7428\n",
    "3 0.8028\n",
    "4 0.8072\n",
    "5 0.7604\n",
    "6 0.7587\n",
    "7 0.7130"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
