{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.special import expit  # Sigmoid function\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return expit(x)    \n",
    "\n",
    "# Define the linear activation function\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x ** 3)))\n",
    "\n",
    "# Define the neural network architecture\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        \"\"\"\n",
    "        Initialize the neural network\n",
    "        :param input_size: number of input neurons\n",
    "        :param hidden_size: number of hidden neurons\n",
    "        :param output_size: number of output neurons\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # # Initialize weights and biases using Xavier initialization\n",
    "        # self.v = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / (input_size + hidden_size))\n",
    "        # self.v0 = np.zeros(hidden_size)\n",
    "        # self.w = np.random.randn(output_size, hidden_size) * np.sqrt(2.0 / (hidden_size + output_size))\n",
    "        # self.w0 = np.zeros(output_size)\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.v = np.random.rand(hidden_size, input_size)\n",
    "        self.v0 = np.random.rand(hidden_size)\n",
    "        self.w = np.random.rand(output_size, hidden_size)\n",
    "        self.w0 = np.random.rand(output_size)\n",
    "\n",
    "    def predict(self, phi: np.ndarray) -> np.ndarray:   \n",
    "        \"\"\"\n",
    "        Make a prediction with the neural network\n",
    "        :param phi: input data\n",
    "        :return: prediction\n",
    "        \"\"\"\n",
    "        hidden_input = np.dot(self.v, phi) + self.v0\n",
    "        hidden_output = relu(hidden_input)\n",
    "        output = np.dot(self.w, hidden_output) + self.w0\n",
    "        return linear(output)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return np.concatenate([self.v.flatten(), self.v0, self.w.flatten(), self.w0])\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        v_end = self.hidden_size * self.input_size\n",
    "        v0_end = v_end + self.hidden_size\n",
    "        w_end = v0_end + (self.output_size * self.hidden_size)\n",
    "\n",
    "        self.v = weights[:v_end].reshape(self.hidden_size, self.input_size)\n",
    "        self.v0 = weights[v_end:v0_end]\n",
    "        self.w = weights[v0_end:w_end].reshape(self.output_size, self.hidden_size)\n",
    "        self.w0 = weights[w_end:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Define the fitness function\n",
    "def fitness(network, data, targets):\n",
    "    predictions = np.array([network.predict(phi) for phi in data])\n",
    "    rmse = root_mean_squared_error(targets, predictions)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSO:\n",
    "    def __init__(self, network, data, targets, num_particles, max_iter, w=0.5, c1=2.0, c2=2.0):\n",
    "        self.network = network\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.num_particles = num_particles\n",
    "        self.max_iter = max_iter\n",
    "        self.w = w\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "\n",
    "        self.dim = len(network.get_weights())\n",
    "        self.swarm = np.random.rand(num_particles, self.dim)\n",
    "        self.velocity = np.zeros((num_particles, self.dim))\n",
    "        self.pbest = self.swarm.copy()\n",
    "        self.lbest = self.swarm.copy()\n",
    "        self.pbest_fitness = np.array([self.evaluate_fitness(weights) for weights in self.swarm])\n",
    "        self.lbest_fitness = self.pbest_fitness.copy()\n",
    "        self.history = []\n",
    "\n",
    "    def evaluate_fitness(self, weights):\n",
    "        self.network.set_weights(weights)\n",
    "        return fitness(self.network, self.data, self.targets)\n",
    "\n",
    "    def calculate_diversity(self):\n",
    "        return np.mean(np.linalg.norm(self.swarm - np.mean(self.swarm, axis=0), axis=1))\n",
    "\n",
    "    def adaptive_neighborhood(self, diversity):\n",
    "        neighborhood_size = min(max(2, int(self.num_particles * (1 - diversity))), self.num_particles)\n",
    "        for i in range(self.num_particles):\n",
    "            distances = np.linalg.norm(self.swarm - self.swarm[i], axis=1)\n",
    "            sorted_indices = np.argsort(distances)\n",
    "            neighborhood_indices = sorted_indices[:neighborhood_size]\n",
    "            best_neighbor_index = np.argmin(self.pbest_fitness[neighborhood_indices])\n",
    "            self.lbest[i] = self.pbest[neighborhood_indices[best_neighbor_index]]\n",
    "            self.lbest_fitness[i] = self.pbest_fitness[neighborhood_indices[best_neighbor_index]]\n",
    "\n",
    "    def optimize(self):\n",
    "        for t in range(self.max_iter):\n",
    "            for i in range(self.num_particles):\n",
    "                self.network.set_weights(self.swarm[i])\n",
    "                current_fitness = fitness(self.network, self.data, self.targets)\n",
    "                \n",
    "                # Update personal best\n",
    "                if current_fitness < self.pbest_fitness[i]:\n",
    "                    self.pbest[i] = self.swarm[i].copy()\n",
    "                    self.pbest_fitness[i] = current_fitness\n",
    "\n",
    "            # Calculate swarm diversity\n",
    "            diversity = self.calculate_diversity()\n",
    "\n",
    "            # Adaptive neighborhood adjustment\n",
    "            self.adaptive_neighborhood(diversity)\n",
    "\n",
    "            for i in range(self.num_particles):\n",
    "                r1, r2 = np.random.rand(self.dim), np.random.rand(self.dim)\n",
    "                self.velocity[i] = (self.w * self.velocity[i] +\n",
    "                                    self.c1 * r1 * (self.pbest[i] - self.swarm[i]) +\n",
    "                                    self.c2 * r2 * (self.lbest[i] - self.swarm[i]))\n",
    "                self.swarm[i] += self.velocity[i]\n",
    "\n",
    "            # Update the best weights found\n",
    "            best_particle = np.argmin(self.pbest_fitness)\n",
    "            best_weights = self.pbest[best_particle]\n",
    "            self.network.set_weights(best_weights)\n",
    "            best_rmse = fitness(self.network, self.data, self.targets)\n",
    "            print(f'Iteration {t+1}/{self.max_iter} - RMSE: {best_rmse}', end='\\r')\n",
    "            self.history.append(best_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "csv_filename = 'Renewable_Energy_Consumption_in_the_US.csv'\n",
    "csv_url = \"https://storage.googleapis.com/kagglesdsdata/datasets/4962496/8352264/dataset.csv?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240531%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240531T131003Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=67056505172956b7c17a6afbde9d66ab941769f53d09951ab578d832886b88c9e9d8cc2eda441b3eb97f0ef863c474c6b742b040e39d242f9410580de0e007f12598eb6fda4ca1d3b01347657507877a0e62d658b42ee2b13147f9fe643d445120d56cc1f8741d9d9e750add7d06a351b811c602f282b7ae632c4cf372138addbad3bb50a6b6a6f9d0596bfd41779c4b901c8265669d00c23e494f669dee5f0c81ebd9ba80360147b480ebe164b3de9679b1ff62490833de99112e01354c3286b3f0118d89625e75092c6711ff6119571a3d0c3f683d3c1cf13a4a648242e435f7a26756e2e0ece1dc62dd47adee56b370a6c78f21dca7b1ab7418e2a4e6cf0d\"\n",
    "\n",
    "# Download the dataset if not available locally\n",
    "if not os.path.exists(csv_filename):\n",
    "    print(\"Downloading the dataset...\")\n",
    "    response = requests.get(csv_url)\n",
    "    with open(csv_filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"CSV file found locally.\")\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading CSV file...\")\n",
    "data = pd.read_csv(csv_filename)\n",
    "print(\"Dataset loaded\")\n",
    "\n",
    "# Preprocess the data\n",
    "print(\"Preprocessing data...\")\n",
    "data = data.dropna(subset=['Total Renewable Energy'])\n",
    "\n",
    "# Convert date to datetime\n",
    "data['dt'] = pd.to_datetime(data[['Year', 'Month']].assign(DAY=1))\n",
    "\n",
    "def create_lag_features(data, lags=1, targets=['Total Renewable Energy']):\n",
    "    df = pd.DataFrame(data)\n",
    "    columns = [df[targets].shift(i) for i in range(lags, 0, -1)]\n",
    "    columns = [df] + columns\n",
    "    df = pd.concat(columns, axis=1)\n",
    "    df.dropna(inplace=True)\n",
    "    # df.columns = ['dt', target] + [f'{target}_lag_{i}' for i in range(1, lags + 1)]\n",
    "    df.columns = ['dt'] + targets + [f'{target}_lag_{i}' for target in targets for i in range(1, lags + 1)]\n",
    "    return df\n",
    "\n",
    "# Creating time series\n",
    "# Using LandAverageTemperatureUncertainty as exogenous variable\n",
    "ts = data[['dt', 'Total Renewable Energy']].dropna().copy()\n",
    "ts = ts.groupby('dt', as_index=False).sum()\n",
    "\n",
    "# Create lag features\n",
    "lags = 5  # Number of lag observations\n",
    "data_lagged = create_lag_features(ts, lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=ts['dt'], y=ts['Total Renewable Energy'], mode='lines', name='Avg Temp'))\n",
    "# fig.add_trace(go.Scatter(x=ts['dt'], y=ts['LandAverageTemperatureUncertainty'], mode='lines', name='Uncertainty'))\n",
    "fig.update_layout(title='Consumo Total de Energia Renovável ', xaxis_title='Data', yaxis_title='Energia Renovável (Tril. de BTUs)')\n",
    "fig.show('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target variable\n",
    "X = data_lagged.drop(['dt', 'Total Renewable Energy'], axis=1)\n",
    "y = data_lagged['Total Renewable Energy']\n",
    "\n",
    "# Split the data using TimeSeriesSplit\n",
    "time_split = TimeSeriesSplit(n_splits=4)\n",
    "train_index, test_index = list(time_split.split(X))[-1]\n",
    "X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "print(f\"Percent of data in training set: {len(X_train) / len(X) * 100:.2f}%\")\n",
    "print(f\"Percent of data in test set: {len(X_test) / len(X) * 100:.2f}%\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "#no scaling\n",
    "# X_train = X_train.to_numpy()\n",
    "# X_test = X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network structure\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 5\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network\n",
    "network = NeuralNetwork(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network using PSO\n",
    "pso = PSO(network, X_train, y_train, num_particles=100, max_iter=300, w=0.7, c1=1.5, c2=2)\n",
    "pso.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([network.predict(phi) for phi in X_test])\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness(network, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_plot_train['dt'] = data_lagged[data_lagged.index < X_plot_train.shape[0]]['dt']\n",
    "X_plot_train['Total Renewable Energy'] = y_train\n",
    "\n",
    "X_plot_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "X_plot_test['dt'] = data_lagged[data_lagged.index >= X_plot_train.shape[0]]['dt'].reset_index(drop=True)\n",
    "X_plot_test['Total Renewable Energy'] = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "X_plot_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_plot_train['dt'] = data_lagged[data_lagged.index < X_plot_train.shape[0]]['dt']\n",
    "X_plot_train['Total Renewable Energy'] = y_train\n",
    "\n",
    "X_plot_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "X_plot_test['dt'] = data_lagged[data_lagged.index >= X_plot_train.shape[0]]['dt'].reset_index(drop=True)\n",
    "X_plot_test['Total Renewable Energy'] = y_test.reset_index(drop=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=X_plot_train['dt'], y=X_plot_train['Total Renewable Energy'], mode='lines', name='Train'))\n",
    "fig.add_trace(go.Scatter(x=X_plot_test['dt'], y=X_plot_test['Total Renewable Energy'], mode='lines', name='Test', opacity=0.8))\n",
    "fig.add_trace(go.Scatter(x=X_plot_test['dt'], y=pd.Series(y_pred.flatten()), mode='lines', name='Pred.'))\n",
    "fig.update_layout(title=\"Predição Energia (PSO | N=3)\", yaxis_title=\"Consumo Energia (Tril. BTUs)\", xaxis_title=\"Data\")\n",
    "fig.show('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history = pd.DataFrame(pso.history, columns=['RMSE'])\n",
    "df_history.to_csv(\"pso_results/PSO_energy_N5_7.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n=3\n",
    "\n",
    "1 34.3523\n",
    "2 26.1027\n",
    "3 30.667304068490356\n",
    "\n",
    "n=5\n",
    "25.891294342815627\n",
    "25.327922808305026\n",
    "30.199094355129624\n",
    "23.028606916511905\n",
    "31.092805546274157\n",
    "24.450476480174938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()   \n",
    "for i in range(1, 6):\n",
    "    df = pd.read_csv(f\"pso_results/PSO_energy_N5_{i}.csv\")\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['RMSE'], mode='lines', name=f'Run {i}'))\n",
    "\n",
    "fig.update_layout(title=\"Iterações PSO (Temperatura | N=5) \", xaxis_title=\"Iteração\", yaxis_title=\"RMSE\")\n",
    "fig.show('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
